@article{Vogel2011,
abstract = {Exposing healthy adults to extended periods of wakefulness is known to induce changes in psychomotor functioning [Maruff (2005). J. Sleep Res. 14, 21-27]. The effect of fatigue on speech is less well understood. To date, no studies have examined the pitch and timing of neurologically healthy individuals over 24 h of sustained wakefulness. Therefore, speech samples were systematically acquired (e.g., every 4 h) from 18 healthy adults over 24 h. Stimuli included automated and extemporaneous speech tasks, sustained vowel, and a read passage. Measures of timing, frequency and spectral energy were derived acoustically using PRAAT and significant changes were observed on all tasks. The effect of fatigue on speech was found to be strongest just before dawn (after 22 h). Specifically, total speech time, mean pause length, and total signal time all increased as a function of increasing levels of fatigue on the reading tasks; percentage pause and mean pause length decreased on the counting task; F4 variation decreased on the sustained vowel tasks /a:/; and alpha ratio increased on the extemporaneous speech tasks. These findings suggest that acoustic methodologies provide objective data on central nervous system functioning and that changes in speech production occur in healthy adults after just 24 h of sustained wakefulness. {\textcopyright} 2010 Acoustical Society of America.},
author = {Vogel, Adam P. and Fletcher, Janet and Maruff, Paul},
doi = {10.1121/1.3506349},
file = {:Users/suzinia/References/Vogel, Fletcher, Maruff{\_}2011{\_}Acoustic analysis of the effects of sustained wakefulness on speech.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {ComParE19},
number = {6},
pages = {3747--3756},
title = {{Acoustic analysis of the effects of sustained wakefulness on speech}},
volume = {128},
year = {2011}
}


@article{Howard2014,
abstract = {Drivers are not always aware that they are becoming impaired as a result of sleepiness. Using specific symptoms of sleepiness might assist with recognition of drowsiness related impairment and help drivers judge whether they are safe to drive a vehicle, however this has not been evaluated. In this study, 20 healthy volunteer professional drivers completed two randomized sessions in the laboratory - one under 24 h of acute sleep deprivation, and one with alcohol. The Psychomotor Vigilance Task (PVT) and a 30 min simulated driving task (AusEdTM) were performed every 3-4 h in the sleep deprivation session, and at a BAC of 0.00{\%} and 0.05{\%} in the alcohol session, while electroencephalography (EEG) and eye movements were recorded. After each test session, drivers completed the Karolinska Sleepiness Scale (KSS) and the Sleepiness Symptoms Questionnaire (SSQ), which includes eight specific sleepiness and driving performance symptoms. A second baseline session was completed on a separate day by the professional drivers and in an additional 20 non-professional drivers for test-retest reliability. There was moderate test-retest agreement on the SSQ (r = 0.59). Significant correlations were identified between individual sleepiness symptoms and the KSS score (r values 0.50-0.74, p {\textless} 0.01 for all symptoms). The frequency of all SSQ items increased during sleep deprivation ($\chi$2 values of 28.4-80.2, p {\textless} 0.01 for all symptoms) and symptoms were related to increased subjective sleepiness and performance deterioration. The symptoms "struggling to keep your eyes open", "difficulty maintaining correct speed", "reactions were slow" and "head dropping down" were most closely related to increased alpha and theta activity on EEG (r values 0.49-0.59, p {\textless} 0.001) and "nodding off to sleep" and "struggling to keep your eyes open" were related to slow eye movements (r values 0.67 and 0.64, p {\textless} 0.001). Symptoms related to visual disturbance and impaired driving performance were most accurate at detecting severely impaired driving performance (AUC on ROC curve of 0.86-0.91 for detecting change in lateral lane position greater than the change at a BAC of 0.05{\%}). Individual sleepiness symptoms are related to impairment during acute sleep deprivation and might be able to assist drivers in recognizing their own sleepiness and ability to drive safely. {\textcopyright} 2013 Published by Elsevier Ltd.},
author = {Howard, Mark E. and Jackson, Melinda L. and Berlowitz, David and O'Donoghue, Fergal and Swann, Philip and Westlake, Justine and Wilkinson, Vanessa and Pierce, Rob J.},
doi = {10.1016/j.aap.2013.09.003},
file = {:Users/suzinia/References/Howard et al.{\_}2014{\_}Specific sleepiness symptoms are indicators of performance impairment during sleep deprivation.pdf:pdf},
issn = {00014575},
journal = {Accident Analysis and Prevention},
keywords = {Driving,Perception,Psychomotor performance,Sleep,Sleep deprivation,Subjective sleepiness},
mendeley-groups = {ComParE19},
pages = {1--8},
publisher = {Elsevier Ltd},
title = {{Specific sleepiness symptoms are indicators of performance impairment during sleep deprivation}},
url = {http://dx.doi.org/10.1016/j.aap.2013.09.003},
volume = {62},
year = {2014}
}


@inproceedings{Park2017,
abstract = {Due to within-speaker variability in phonetic content and/or speaking style, the performance of automatic speaker verifica-tion (ASV) systems degrades especially when the enrollment and test utterances are short. This study examines how dif-ferent types of variability influence performance of ASV sys-tems. Speech samples ({\textless} 2 sec) from the UCLA Speaker Vari-ability Database containing 5 different read sentences by 200 speakers were used to study content variability. Other samples (about 5 sec) that contained speech directed towards pets, char-acterized by exaggerated prosody, were used to analyze style variability. Using the i-vector/PLDA framework, the ASV sys-tem error rate with MFCCs had a relative increase of at least 265{\%} and 730{\%} in content-mismatched and style-mismatched trials, respectively. A set of features that represents voice qual-ity (F0, F1, F2, F3, H1-H2, H2-H4, H4-H2k, A1, A2, A3, and CPP) was also used. Using score fusion with MFCCs, all con-ditions saw decreases in error rates. In addition, using the NIST SRE10 database, score fusion provided relative improvements of 11.78{\%} for 5-second utterances, 12.41{\%} for 10-second ut-terances, and a small improvement for long utterances (about 5 min). These results suggest that voice quality features can improve short-utterance text-independent ASV system perfor-mance.},
address = {Stockholm, Sweden},
author = {Park, Soo Jin and Yeung, Gary and Kreiman, Jody and Keating, Patricia A. and Alwan, Abeer},
booktitle = {Proc. Interspeech},
doi = {10.21437/Interspeech.2017-157},
file = {:Users/suzinia/References//Park et al.{\_}2017{\_}Using Voice Quality Features to Improve Short-Utterance, Text-Independent Speaker Verification Systems.pdf:pdf},
keywords = {speaker recognition,voice quality},
mendeley-groups = {BIB{\_}JASA17,BIB{\_}ODYSSEY18,BIB{\_}IS18,Defense,Dissertation},
mendeley-tags = {speaker recognition,voice quality},
month = {aug},
pages = {1522--1526},
title = {{Using Voice Quality Features to Improve Short-Utterance, Text-Independent Speaker Verification Systems}},
year = {2017}
}
@article{Park2018JASA,
author = {Park, Soo Jin and Yeung, Gary and Vesselinova, Neda and Kreiman, Jody and Keating, Patricia A. and Alwan, Abeer},
doi = {10.1121/1.5045323},
file = {:Users/suzinia/References/Park et al.{\_}2018{\_}Towards understanding speaker discrimination abilities in humans and machines for text-independent short utterances of.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {BIB{\_}ODYSSEY18,Defense,Dissertation},
month = {jul},
number = {1},
pages = {375--386},
title = {{Towards Understanding Speaker Discrimination Abilities in Humans and Machines for Text-Independent Short Utterances of Different Speech Styles}},
url = {http://asa.scitation.org/doi/10.1121/1.5045323},
volume = {144},
year = {2018}
}


@article{DeJong2009,
abstract = {In this article, we describe a method for automatically detecting syllable nuclei in order to measure speech rate without the need for a transcription. A script written in the software program Praat (Boersma {\&} Weenink, 2007) detects syllables in running speech. Peaks in intensity (dB) that are preceded and followed by dips in intensity are considered to be potential syllable nuclei. The script subsequently discards peaks that are not voiced. Testing the resulting syllable counts of this script on two corpora of spoken Dutch, we obtained high correlations between speech rate calculated from human syllable counts and speech rate calculated from automatically determined syllable counts. We conclude that a syllable count measured in this automatic fashion suffices to reliably assess and compare speech rates between participants and tasks.},
author = {de Jong, Nivja H. and Wempe, Ton},
doi = {10.3758/BRM.41.2.385},
file = {:Users/suzinia/References/de Jong, Wempe{\_}2009{\_}Praat script to detect syllable nuclei and measure speech rate automatically.pdf:pdf},
issn = {1554351X},
journal = {Behavior Research Methods},
mendeley-groups = {ComParE19},
number = {2},
pages = {385--390},
title = {{Praat script to detect syllable nuclei and measure speech rate automatically}},
volume = {41},
year = {2009}
}


@article{Roehrs1989,
abstract = {Twenty-four healthy, young (21-35 years old) men with no complaints of daytime sleepiness, no habitual napping, and polysomnographically verified normal nocturnal sleep extended their time in bed (TIB) to 10 h for 6 consecutive nights to assess the effects of sleep extension on daytime sleepiness and performance. Twelve subjects had basal average daily sleep latencies of less than or equal to 6 min on the Multiple Sleep Latency Test and 12 had latencies of greater than or equal to 16 min before TIB was extended. The sleep extension improved daytime sleepiness differentially in the two groups. The degree of improvement was greater in the sleepy subjects than the alert subjects and the pattern of improvement differed between the groups. Sleepy subjects showed an immediate and uniform increase in alertness, while alert subjects did not show improvements until late in the extension. However, sleepy subjects never achieved the baseline level of sleepiness/alertness seen in the alert subjects.},
author = {Roehrs, T. and Timms, V. and Zwyghuizen-Doorenbos, A. and Roth, T.},
doi = {10.1093/sleep/12.5.449},
file = {:Users/suzinia/References/Roehrs et al.{\_}1989{\_}Sleep extension in sleepy and alert normals.pdf:pdf},
issn = {01618105},
journal = {Sleep},
mendeley-groups = {ComParE19},
number = {5},
pages = {449--457},
pmid = {2799218},
title = {{Sleep extension in sleepy and alert normals}},
volume = {12},
year = {1989}
}
@article{Greeley2007,
abstract = {In the present article, we present a means to remotely and transparently estimate an individual's level of fatigue by quantifying changes in his or her voice characteristics. Using Voice analysis to estimate fatigue is unique from established cognitive measures in a number of ways: (1) speaking is a natural activity requiring no initial training or learning curve, (2) voice recording is a unobtrusive operation allowing the speakers to go about their normal work activities, (3) using telecommunication infrastructure (radio, telephone, etc.) a diffuse set of remote populations can be monitored at a central location, and (4) often, previously recorded voice data are available for post hoc analysis. By quantifying changes in the mathematical coefficients that describe the human speech production process, we were able to demonstrate that for speech sounds requiring a large average air flow, a speaker's voice changes in synchrony with both direct measures of fatigue and with changes predicted by the length of time awake.},
author = {Greeley, Harold P. and Berg, Joel and Friets, Eric and Wilson, John and Greenough, Glen and Picone, Joseph and Whitmore, Jeffrey and Nesthus, Thomas},
doi = {10.3758/BF03193033},
file = {:Users/suzinia/References/Greeley et al.{\_}2007{\_}Fatigue estimation using voice analysis.pdf:pdf},
issn = {1554351X},
journal = {Behavior Research Methods},
mendeley-groups = {ComParE19},
number = {3},
pages = {610--619},
title = {{Fatigue estimation using voice analysis}},
volume = {39},
year = {2007}
}
@article{W.1993,
author = {W., Picone Joseph},
file = {:Users/suzinia/References/W.{\_}1993{\_}Signal modeling techniques in speech recognition.pdf:pdf},
journal = {Proceedings of the IEEE},
mendeley-groups = {ComParE19},
number = {9},
pages = {1215--1247},
title = {{Signal modeling techniques in speech recognition}},
volume = {81},
year = {1993}
}
@article{Florian2011,
author = {Florian, H and Batliner, Anton and Elmar, N and Schnieder, Sebastian and Krajewski, Jarek},
file = {:Users/suzinia/References/Florian et al.{\_}2011{\_}Acoustic-Prosodic Characteristics of Sleepy Speech – Between Performance.pdf:pdf},
mendeley-groups = {ComParE19},
number = {444},
title = {{Acoustic-Prosodic Characteristics of Sleepy Speech – Between Performance}},
year = {2011}
}
@article{Krajewski2007,
author = {Krajewski, Jarek and Kr{\"{o}}ger, Bernd},
file = {:Users/suzinia/References/Krajewski, Kr{\"{o}}ger{\_}2007{\_}Using Prosodic and Spectral Characteristics for Sleepiness Detection Work and Organizational Psychology , 42097.pdf:pdf},
journal = {Changes},
mendeley-groups = {ComParE19},
pages = {1841--1844},
title = {{Using Prosodic and Spectral Characteristics for Sleepiness Detection Work and Organizational Psychology , 42097 Wuppertal , Germany Aachen and Aachen University}},
year = {2007}
}
@article{Krajewski,
author = {Krajewski, Jarek and Schnieder, Sebastian and Monschau, Christopher and Titt, Raphael and Sommer, David and Golz, Martin},
file = {:Users/suzinia/References/Krajewski et al.{\_}Unknown{\_}Large Sleepy Reading Corpus ( LSRC ) Applying Read Speech for Detecting Sleepiness 1 Introduction 2 Large Sleep.pdf:pdf},
mendeley-groups = {ComParE19},
pages = {1--4},
title = {{Large Sleepy Reading Corpus ( LSRC ): Applying Read Speech for Detecting Sleepiness 1 Introduction 2 Large Sleepy Reading Corpus 3 Feature Extraction}}
}


@inproceedings{Park2018IS,
address = {Hyderabad, India},
author = {Park, Soo Jin and Afshan, Amber and Chua, Zhi Ming and Alwan, Abeer},
booktitle = {Proc. Interspeech},
doi = {10.21437/Interspeech.2018-1401},
mendeley-groups = {Defense,Dissertation},
month = {sep},
pages = {157--161},
publisher = {ISCA},
title = {{Using Voice Quality Supervectors for Affect Identification}},
url = {http://www.isca-speech.org/archive/Interspeech{\_}2018/abstracts/1401.html},
year = {2018}
}

@article{dehak2011front,
  title={Front-end factor analysis for speaker verification},
  author={Dehak, Najim and Kenny, Patrick J and Dehak, R{\'e}da and Dumouchel, Pierre and Ouellet, Pierre},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={19},
  number={4},
  pages={788--798},
  year={2011},
  publisher={IEEE}
}

@article{afshan2018effectiveness,
  title={Effectiveness of Voice Quality Features in Detecting Depression},
  author={Afshan, Amber and Guo, Jinxi and Park, Soo Jin and Ravi, Vijay and Flint, Jonathan and Alwan, Abeer},
  journal={Proc. Interspeech 2018},
  pages={1676--1680},
  year={2018}
}

@article{kenny2005eigenvoice,
  title={Eigenvoice modeling with sparse training data},
  author={Kenny, Patrick and Boulianne, Gilles and Dumouchel, Pierre},
  journal={IEEE transactions on speech and audio processing},
  volume={13},
  number={3},
  pages={345--354},
  year={2005},
  publisher={IEEE}
}

@inproceedings{eyben2010opensmile,
  title={Opensmile: the munich versatile and fast open-source audio feature extractor},
  author={Eyben, Florian and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn},
  booktitle={Proceedings of the 18th ACM international conference on Multimedia},
  pages={1459--1462},
  year={2010},
  organization={ACM}
}

@article{hauke2011comparison,
  title={Comparison of values of Pearson's and Spearman's correlation coefficients on the same sets of data},
  author={Hauke, Jan and Kossowski, Tomasz},
  journal={Quaestiones geographicae},
  volume={30},
  number={2},
  pages={87--93},
  year={2011},
  publisher={Versita}
}

@article{rocha2018influence,
  title={The influence of sleep disorders on voice quality},
  author={Rocha, Bruna Rainho and Behlau, Mara},
  journal={Journal of Voice},
  volume={32},
  number={6},
  pages={771--e1},
  year={2018},
  publisher={Elsevier}
}

@article{whitmore1996speech,
  title={Speech during sustained operations},
  author={Whitmore, Jeffrey and Fisher, Stanley},
  journal={Speech Communication},
  volume={20},
  number={1-2},
  pages={55--70},
  year={1996},
  publisher={Elsevier}
}
@article{krajewski2009acoustic,
  title={Acoustic sleepiness detection: Framework and validation of a speech-adapted pattern recognition approach},
  author={Krajewski, Jarek and Batliner, Anton and Golz, Martin},
  journal={Behavior research methods},
  volume={41},
  number={3},
  pages={795--804},
  year={2009},
  publisher={Springer}
}

@article{Kreiman2014,
abstract = {At present, two important questions about voice remain unanswered: When voice quality changes, what physiological alteration caused this change, and if a change to the voice production system occurs, what change in perceived quality can be expected? We argue that these questions can only be answered by an integrated model of voice linking production and perception, and we describe steps towards the development of such a model. Preliminary evidence in support of this approach is also presented. We conclude that development of such a model should be a priority for scientists interested in voice, to explain what physical condition(s) might underlie a given voice quality, or what voice quality might result from a specific physical configuration.},
author = {Kreiman, Jody and Gerratt, Bruce R. and Garellek, Marc and Samlan, Robin and Zhang, Zhaoyan},
doi = {10.3989/loquens.2014.009},
file = {:Users/suzinia/Library/Application Support/Mendeley Desktop/Downloaded/Kreiman et al. - 2014 - Toward a unified theory of voice production and perception(2).pdf:pdf},
isbn = {2386-2637},
issn = {2386-2637},
journal = {Loquens},
keywords = {- en la actualidad,1,2,JND,a saber,acoustics,ac{\'{u}}stica,alteraci{\'{o}}n en el mecanismo,cambio en el sistema,contestar dos cuestiones importantes,cualidad de voz,cuando la cualidad de,de la producci{\'{o}}n y,de producci{\'{o}}n de la,hacia una teor{\'{i}}a unificada,la percepci{\'{o}}n de la,la voz cambia,modeling,modelo,producci{\'{o}}n de voz,quedan por,qu{\'{e}},relacionadas con la voz,resumen,si se produce un,synthesis,s{\'{i}}ntesis,vocal es la responsable,voice production,voice quality,voz,y},
mendeley-groups = {BIB{\_}JASA17,BIB{\_}ODYSSEY18,BIB{\_}IS18},
mendeley-tags = {JND,voice quality},
month = {jun},
number = {1},
pages = {1--9},
title = {{Toward a Unified Theory of Voice Production and Perception}},
volume = {1},
year = {2014}
}
@inproceedings{Li2007,
address = {Honolulu, Hawaii, USA},
author = {Li, Xi and Tao, Jidong and Johnson, Michael T. and Soltis, Joseph and Savage, Anne and Leong, Kirsten M. and Newman, John D.},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2007.367261},
file = {:Users/suzinia/Downloads/04218292.pdf:pdf},
isbn = {1-4244-0727-3},
mendeley-groups = {BIB{\_}IS18},
pages = {IV--1081--IV--1084},
title = {{Stress and Emotion Classification using Jitter and Shimmer Features}},
year = {2007}
}

@article{Garellek2016,
abstract = {A psychoacoustic model of the voicesourcespectrum is proposed. The model is characterized by four spectral slope parameters: the difference in amplitude between the first two harmonics (H1–H2), the second and fourth harmonics (H2–H4), the fourth harmonic and the harmonic nearest 2 kHz in frequency (H4–2 kHz), and the harmonic nearest 2 kHz and that nearest 5 kHz (2 kHz–5 kHz). As a step toward model validation, experiments were conducted to establish the acoustic and perceptual independence of these parameters. In experiment 1, the model was fit to a large number of voicesources. Results showed that parameters are predictable from one another, but that these relationships are due to overall spectral roll-off. Two additional experiments addressed the perceptual independence of the source parameters. Listener sensitivity to H1–H2, H2–H4, and H4–2 kHz did not change as a function of the slope of an adjacent component, suggesting that sensitivity to these components is robust. Listener sensitivity to changes in spectral slope from 2 kHz to 5 kHz depended on complex interactions between spectral slope, spectral noise levels, and H4–2 kHz. It is concluded that the four parameters represent non-redundant acoustic and perceptual aspects of voice quality.},
author = {Garellek, Marc and Samlan, Robin and Gerratt, Bruce R. and Kreiman, Jody},
file = {:C$\backslash$:/Users/mehhe/Desktop/References/Modeling the Voice Source in Terms of Spectral Slopes.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {1404--1410},
title = {{Modeling the voice source in terms of spectral slopes}},
volume = {139},
year = {2016}
}
@article{Moore2004,
abstract = {Human communication is saturated with emotional context that aids in interpreting a speakers mental state. Speech analysis research involving the classification of emotional states has been studied primarily with prosodic (e.g., pitch, energy, speaking rate) and/or spectral (e.g., formants) features. Glottal waveform features, while receiving less attention (due primarily to the difficulty of feature extraction), have also shown strong clustering potential of various emotional and stress states. This study provides a comparison of the major categories of speech analysis in the application of identifying and clustering feature statistics from a control group and a patient group suffering from a clinical diagnosis of depression.},
author = {Moore, E.I.I. and Clements, M. and Peifer, J. and Weisser, L.},
file = {:C$\backslash$:/Users/mehhe/Desktop/References/proposal{\_}116.pdf:pdf},
isbn = {0-7803-8439-3},
issn = {1557-170X},
journal = {The 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
keywords = {affect,depression,emotion,speech analysis},
pages = {17--20},
pmid = {17271592},
title = {{Comparing objective feature statistics of speech for classifying clinical depression}},
volume = {3},
year = {2004}
}
@article{DeKrom1993,
author = {de Krom, Guus},
journal = {Journal of Speech, Language, and Hearing Research},
pages = {254--266},
title = {{A cepstrum-based technique for determining a harmonics-to-noise ratio in speech signals}},
volume = {36},
year = {1993}
}

@inproceedings{kreiman2015relationship,
  title={The relationship between acoustic and perceived intraspeaker variability in voice quality},
  author={Kreiman, Jody and Park, Soo Jin and Keating, Patricia A and Alwan, Abeer},
  booktitle={Sixteenth Annual Conference of the International Speech Communication Association},
  year={2015}
}


@inproceedings{Park2016,
abstract = {Despite recent breakthroughs in automatic speaker recognition (ASpR), system performance still degrades when utterances are short and/or when within-speaker variability is large. This study used short test utterances (2-3sec) to investigate the effect of within-speaker variability on state-of-the-art ASpR system per-formance. A subset of a newly-developed UCLA database is used, which contains multiple speech tasks per speaker. The short utterances combined with a speaking-style mismatch be-tween read sentences and spontaneous affective speech de-graded system performance, for 25 female speakers, by 36{\%}. Because humans are more robust to utterance length or within-speaker variability, understanding human perception might ben-efit ASpR systems. Perception experiments were conducted with recorded read sentences from 3 female speakers, and a model is proposed to predict the perceptual dissimilarity be-tween tokens. Results showed that a set of voice quality fea-tures including F0, F1, F2, F3, H1*-H2*, H2*-H4*, H4*-H2k*, H2k*-H5k, and CPP provides information that complements MFCCs. By fusing the feature set with MFCCs, human re-sponse prediction RMS error was .12, which represents a 12{\%} relative error reduction compared to using MFCCs alone. In ASpR experiments with short utterances from 50 speakers, the voice quality feature set decreased the error rate by 11{\%} when fused with MFCCs.},
address = {San Francisco, USA},
author = {Park, Soo Jin and Sigouin, Caroline and Kreiman, Jody and Keating, Patricia A. and Guo, Jinxi and Yeung, Gary and Kuo, Fang-Yu and Alwan, Abeer},
booktitle = {Proc. Interspeech},
doi = {10.21437/Interspeech.2016-523},
file = {:Users/suzinia/References//Park et al.{\_}2016{\_}Speaker Identity and Voice Quality Modeling Human Responses and Automatic Speaker Recognition.pdf:pdf},
keywords = {speaker perception,speaker recognition,speech perception model,voice quality},
mendeley-groups = {BIB{\_}JASA17,BIB{\_}ODYSSEY18,BIB{\_}IS18,Defense,Dissertation},
mendeley-tags = {speaker perception,speaker recognition,voice quality},
month = {sep},
pages = {1044--1048},
title = {{Speaker Identity and Voice Quality: Modeling Human Responses and Automatic Speaker Recognition}},
year = {2016}
}

@article{aakerstedt1990subjective,
  title={Subjective and objective sleepiness in the active individual},
  author={{\AA}kerstedt, Torbj{\"o}rn and Gillberg, Mats},
  journal={International Journal of Neuroscience},
  volume={52},
  number={1-2},
  pages={29--37},
  year={1990},
  publisher={Taylor \& Francis}
}

@inproceedings{you2004entropy,
  title={Entropy-based variable frame rate analysis of speech signals and its application to ASR},
  author={You, Hong and Zhu, Qifeng and Alwan, Abeer},
  booktitle={2004 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={1},
  pages={I--549},
  year={2004},
  organization={IEEE}
}
@article{brookes1997voicebox,
  title={Voicebox: Speech processing toolbox for matlab},
  author={Brookes, Mike and others},
  journal={Software, available [Mar. 2011] from www. ee. ic. ac. uk/hp/staff/dmb/voicebox/voicebox. html},
  volume={47},
  year={1997}
}

@article{Hansen2015,
abstract = {Identifying a person by his or her voice is an important human trait most take for granted in natural human-to-human interaction/communication. Speaking to someone over the telephone usually begins by identifying who is speaking and, at least in cases of familiar speakers, a subjective verification by the listener that the identity is correct and the conversation can proceed. Automatic speaker-recognition systems have emerged as an important means of verifying identity in many e-commerce applications as well as in general business interactions, forensics, and law enforcement. Human experts trained in forensic speaker recognition can perform this task even better by examining a set of acoustic, prosodic, and linguistic characteristics of speech in a general approach referred to as structured listening. Techniques in forensic speaker recognition have been developed for many years by forensic speech scientists and linguists to help reduce any potential bias or preconceived understanding as to the validity of an unknown audio sample and a reference template from a potential suspect. Experienced researchers in signal processing and machine learning continue to develop automatic algorithms to effectively perform speaker recognition?with ever-improving performance?to the point where automatic systems start to perform on par with human listeners. In this article, we review the literature on speaker recognition by machines and humans, with an emphasis on prominent speaker-modeling techniques that have emerged in the last decade for automatic systems. We discuss different aspects of automatic systems, including voice-activity detection (VAD), features, speaker models, standard evaluation data sets, and performance metrics. Human speaker recognition is discussed in two parts?the first part involves forensic speaker-recognition methods, and the second illustrates how a na?ve listener performs this task from a neuroscience perspective. We conclude this review with a comparative- study of human versus machine speaker recognition and attempt to point out strengths and weaknesses of each.},
author = {Hansen, John H. L. and Hasan, Taufiq},
doi = {10.1109/MSP.2015.2462851},
file = {:Users/suzinia/Library/Application Support/Mendeley Desktop/Downloaded/Hansen, Hasan - 2015 - Speaker Recognition by Machines and Humans A Tutorial Review(2).pdf:pdf},
isbn = {1053-5888 VO - 32},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {Audio systems,Authentication,Feature extraction,Forensics,Noise measurement,Speaker recognition,Speech recognition,audio sample,automatic algorithms,performance metrics,reference template,speaker models,speaker recognition,standard evaluation data sets,structured listening,subjective verification,voice-activity detection},
mendeley-groups = {BIB{\_}JASA17,BIB{\_}IS18},
mendeley-tags = {speaker recognition},
number = {6},
pages = {74--99},
title = {{Speaker Recognition by Machines and Humans: A Tutorial Review}},
volume = {32},
year = {2015}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{schullerinterspeech,
  title={The INTERSPEECH 2019 Computational Paralinguistics Challenge: Styrian Dialects, Continuous Sleepiness, Baby Sounds \& Orca Activity},
  author={Schuller, Bj{\"o}rn W and Batliner, Anton and Bergler, Christian and Pokorny, Florian B and Krajewski, Jarek and Cychosz, Margaret and Vollmann, Ralf and Roelen, Sonja-Dana and Schnieder, Sebastian and Bergelson10, Elika and others},
  journal={Interspeech},
  year={2019}
}

@article{de2019speech,
  title={Speech Analysis for Fatigue and Sleepiness Detection of a Pilot},
  author={de Vasconcelos, Carla Aparecida and Vieira, Maur{\'\i}lio Nunes and Kecklund, G{\"o}ran and Yehia, Hani Camille},
  journal={Aerospace Medicine and Human Performance},
  volume={90},
  number={4},
  pages={415--418},
  year={2019},
  publisher={Aerospace Medical Association}
}

@inproceedings{alvin2004nist,
  title={NIST speaker recognition evaluation chronicles},
  author={Alvin, Mark Przybocki and Martin, Alvin},
  booktitle={Proc. Odyssey 2004, The Speaker and Language Recognition Workshop},
  year={2004},
  organization={Citeseer}
}

@inproceedings{przybocki2006nist,
  title={NIST speaker recognition evaluation chronicles-part 2},
  author={Przybocki, Mark A and Martin, Alvin F and Le, Audrey N},
  booktitle={2006 IEEE Odyssey-The Speaker and Language Recognition Workshop},
  pages={1--6},
  year={2006},
  organization={IEEE}
}

@article{weninger2013acoustics,
  title={On the acoustics of emotion in audio: what speech, music, and sound have in common},
  author={Weninger, Felix and Eyben, Florian and Schuller, Bj{\"o}rn W and Mortillaro, Marcello and Scherer, Klaus R},
  journal={Frontiers in psychology},
  volume={4},
  pages={292},
  year={2013},
  publisher={Frontiers}
}
@inproceedings{martin2009nist,
  title={NIST 2008 speaker recognition evaluation: Performance across telephone and room microphone channels},
  author={Martin, Alvin F and Greenberg, Craig S},
  booktitle={Tenth Annual Conference of the International Speech Communication Association},
  year={2009}
}
@inproceedings{rahman2011detecting,
  title={Detecting sleepiness by fusing classifiers trained with novel acoustic features},
  author={Rahman, Tauhidur and Mariooryad, Soroosh and Keshavamurthy, Shalini and Liu, Gang and Hansen, John HL and Busso, Carlos},
  booktitle={Twelfth Annual Conference of the International Speech Communication Association},
  year={2011}
}
@inproceedings{bone2011intoxicated,
  title={Intoxicated speech detection by fusion of speaker normalized hierarchical features and GMM supervectors},
  author={Bone, Daniel and Black, Matthew P and Li, Ming and Metallinou, Angeliki and Lee, Sungbok and Narayanan, Shrikanth},
  booktitle={Twelfth Annual Conference of the International Speech Communication Association},
  year={2011}
}

@article{pisoni1989effects,
  title={Effects of alcohol on the acoustic-phonetic properties of speech: perceptual and acoustic analyses},
  author={Pisoni, David B and Martin, Christopher S},
  journal={Alcoholism: Clinical and Experimental Research},
  volume={13},
  number={4},
  pages={577--587},
  year={1989},
  publisher={Wiley Online Library}
}

@article{graff1999switchboard,
  title={Switchboard-2 phase ii},
  author={Graff, D and Walker, K and Canavan, A},
  journal={LDC 99S79--http://www. ldc. upenn. edu/Catalog},
  year={1999}
}

@book{shue2010voice,
  title={The voice source in speech production: Data, analysis and models},
  author={Shue, Yen-Liang},
  year={2010},
  publisher={University of California, Los Angeles}
}

